{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce5f0576-07d9-4009-92f3-a89745291288",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import cv2\n",
    "from time import sleep\n",
    "from datetime import datetime\n",
    "import os\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from pandas import read_csv, read_excel, DataFrame\n",
    "\n",
    "from skimage.feature import local_binary_pattern as LBP\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.experimental import enable_halving_search_cv # noqa\n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from utils_segmentation import get_features, p3, p0, p00, n_components, plots, cornfusion\n",
    "\n",
    "import pickle\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678192f1-78a4-41df-becb-943cf690d358",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for version in ['V2','V1']:\n",
    "    print(version)\n",
    "    p1 = os.path.join('../../','work','CookIRCamET','Images','CookHY2023',version,'TifPng','RGB')\n",
    "    p2 = os.path.join('../../','work','CookIRCamET','Images','CookHY2023',version,'TifPng')\n",
    "    p11 = os.path.join('../../','work','CookIRCamET','Images','CprlHY2023',version,'TifPng','RGB')\n",
    "    p22 = os.path.join('../../','work','CookIRCamET','Images','CprlHY2023',version,'TifPng')\n",
    "    f_imgs=[]\n",
    "    imgs=[]\n",
    "    n_img=0\n",
    "    for di,do in zip([p1,p11],[p2,p22]):\n",
    "        fs=os.listdir(di)\n",
    "        print(di)\n",
    "        print(do)\n",
    "        shuffle(fs)\n",
    "        for f in fs:\n",
    "            if 'bgr' in f:\n",
    "    \n",
    "                f_imgs = np.append(f_imgs,f)\n",
    "                print(f)\n",
    "                filepath = os.path.join(di,f)\n",
    "                bgr = cv2.imread(filepath,cv2.IMREAD_UNCHANGED)\n",
    "                time_place = f.split('_bgr.')[0].split('_')\n",
    "    \n",
    "                labels = False\n",
    "                f_labels = os.path.join(do,'SunShade',f.split('_bgr')[0]+'_class2.tif')\n",
    "                if (os.path.exists(f_labels)):\n",
    "                    labels1 = cv2.imread(f_labels,cv2.IMREAD_UNCHANGED)\n",
    "                    labels = True\n",
    "    \n",
    "                f_labels = os.path.join(do,'SoilResVegSnow',f.split('_bgr')[0]+'_class4.tif')\n",
    "                if (os.path.exists(f_labels)):\n",
    "                    labels2 = cv2.imread(f_labels,cv2.IMREAD_UNCHANGED)\n",
    "                    labels2[labels2==4]=3#flowers->veg\n",
    "                    labels = (True & labels)\n",
    "                    if labels: \n",
    "                        #8-class\n",
    "                        labels3 = 4*labels1+labels2\n",
    "                        if not os.path.exists(os.path.join(do,'Masks')): os.mkdir(os.path.join(do,'Masks'))\n",
    "                        cv2.imwrite(os.path.join(do,'Masks',f.split('_bgr')[0]+'_class8.png'),labels3)\n",
    "    \n",
    "                    feat,_ = get_features(bgr)\n",
    "                    labels1 = labels1.ravel()        \n",
    "                    labels2 = labels2.ravel() \n",
    "                    labels3 = labels3.ravel() \n",
    "    \n",
    "                    if not np.any(np.isnan(feat)):\n",
    "                        imgs.append({'bgr':bgr,'feats':feat,'labels1':labels1,'labels2':labels2,'labels3':labels3})\n",
    "                        n_img=n_img+1\n",
    "    \n",
    "    n_feat = feat.shape[1]\n",
    "    \n",
    "    feats_raw = []\n",
    "    labels3 = []\n",
    "    for sample in imgs:\n",
    "        feats_raw.append(sample['feats'])\n",
    "        labels3.append(sample['labels3'])\n",
    "    del imgs\n",
    "    \n",
    "    feats_raw = np.array(feats_raw).reshape((-1,n_feat)).astype(np.float32)\n",
    "    labels3 = np.array(labels3).reshape((-1,1)).astype(np.int32).ravel()\n",
    "    train_feats, test_feats, train_labels, test_labels = train_test_split(feats_raw, labels3, test_size=0.2, random_state=42)\n",
    "    for model in ['mlp']:\n",
    "       #Pipeline\n",
    "        #initial scaling\n",
    "        scaler = StandardScaler()\n",
    "        #Best parameter (CV score=0.892): mlp v1\n",
    "        #{'clf__activation': 'logistic', 'clf__hidden_layer_sizes': (432, 117, 32), 'pca__n_components': 0.999}\n",
    "        #0.8922358777029111 0.8923369140625\n",
    "        #pca\n",
    "        pca = PCA(svd_solver='full',n_components=0.999)\n",
    "        clf = MLPClassifier(max_iter=1000)\n",
    "        if model == 'mlp':\n",
    "            #mlp\n",
    "            clf = MLPClassifier(max_iter=1000)\n",
    "            #tune hyperparameters\n",
    "            layers = []\n",
    "    \n",
    "            #for layer1 in [2,4,8,16]:\n",
    "            # for layer1 in [8,16]:\n",
    "            #     for layer2 in [4]:\n",
    "            #         layer = (layer1*n_feat//2, int(np.sqrt(n_feat//2*n_components*layer1*layer2)), layer2*n_components)\n",
    "            for layer1 in [4,8]:\n",
    "                for layer2 in [2,4]:\n",
    "                    layer = (layer1*n_feat,np.sqrt(layer1*n_feat*layer2*n_components),layer2*n_components)\n",
    "                    layers.append(layer)\n",
    "    \n",
    "            #parameters = {'pca__n_components':(0.99,0.999),'clf__activation':('relu','logistic'),'clf__hidden_layer_sizes':layers}\n",
    "            parameters = {'clf__hidden_layer_sizes':layers}\n",
    "        \n",
    "        elif model == 'hgbc':\n",
    "            clf = HistGradientBoostingClassifier(max_iter=1000)\n",
    "        \n",
    "           # parameters = {'pca__n_components':(0.99,0.999),'clf__min_samples_leaf':(20,200,2000,20000),'clf__max_bins':(31,63,127,255)}\n",
    "            parameters = {'clf__max_bins':(31,63,127,255)}\n",
    "        pipeline = Pipeline(steps=[(\"scaler\", scaler), (\"pca\", pca), (\"clf\", clf)])\n",
    "        \n",
    "        search = HalvingGridSearchCV(pipeline, parameters,n_jobs=-1,cv=5,verbose=3,aggressive_elimination=True)\n",
    "        \n",
    "        search.fit(train_feats, train_labels)\n",
    "        print(\"Best parameter (CV score=%0.3f):\" % search.best_score_)\n",
    "        print(search.best_params_)\n",
    "        filename = os.path.join(p3,'model_pipeline_'+version+'_'+model+'_'+datestr+'_final.pk.sav')\n",
    "        with open(filename, 'wb') as f:  # Python 3: open(..., 'wb'\n",
    "            pickle.dump(search.best_estimator_, f)\n",
    "            \n",
    "        pred = search.predict(test_feats)\n",
    "\n",
    "\n",
    "        M,f,a = cornfusion(test_labels,pred,n_components)\n",
    "        \n",
    "        plt.matshow(M)\n",
    "        plt.ylabel(\"Predicted\")\n",
    "        plt.xlabel(\"Observed\")\n",
    "        plt.title(version+\" Confusion Matrix\")\n",
    "        plt.savefig(os.path.join(p3,'m_'+version+'_'+model+'_'+datestr+'_final.png'),dpi=300)\n",
    "        \n",
    "        print(f,a)\n",
    "        \n",
    "        M_df = {}\n",
    "        M_df['sun_soil'] = M[:,0]\n",
    "        M_df['sun_res'] = M[:,1]\n",
    "        M_df['sun_can'] = M[:,2]\n",
    "        M_df['sun_snow'] = M[:,3]\n",
    "        M_df['shade_soil'] = M[:,4]\n",
    "        M_df['shade_res'] = M[:,5]\n",
    "        M_df['shade_can'] = M[:,6]\n",
    "        M_df['shade_snow'] = M[:,7]\n",
    "        M_df = DataFrame(M_df)\n",
    "        M_df.to_csv(os.path.join(p3,'M_'+version+'_'+model+'_'+datestr+'_final.csv'))\n",
    "        \n",
    "        p_df = DataFrame(search.best_params_)\n",
    "        p_df.to_csv(os.path.join(p3,'params_'+version+'_'+model+'_'+datestr+'_final.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1c8fc4-302d-48ba-a693-4a332c8e4f3e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iterations: 2\n",
      "n_required_iterations: 2\n",
      "n_possible_iterations: 2\n",
      "min_resources_: 22118400\n",
      "max_resources_: 66355200\n",
      "aggressive_elimination: True\n",
      "factor: 3\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 4\n",
      "n_resources: 22118400\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/project/nsaru/nsaru-cv/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:693: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n",
      "/project/nsaru/nsaru-cv/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:693: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n",
      "/project/nsaru/nsaru-cv/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:693: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n",
      "/project/nsaru/nsaru-cv/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:693: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n",
      "/project/nsaru/nsaru-cv/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:693: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n",
      "/project/nsaru/nsaru-cv/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:693: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n",
      "/project/nsaru/nsaru-cv/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:693: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n",
      "/project/nsaru/nsaru-cv/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:693: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n",
      "/project/nsaru/nsaru-cv/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:693: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n",
      "/project/nsaru/nsaru-cv/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:693: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n",
      "/project/nsaru/nsaru-cv/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:693: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n",
      "/project/nsaru/nsaru-cv/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:693: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n",
      "/project/nsaru/nsaru-cv/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:693: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n",
      "/project/nsaru/nsaru-cv/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:693: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n",
      "/project/nsaru/nsaru-cv/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:693: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n",
      "/project/nsaru/nsaru-cv/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:693: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n",
      "/project/nsaru/nsaru-cv/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:693: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n",
      "/project/nsaru/nsaru-cv/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:693: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n",
      "/project/nsaru/nsaru-cv/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:693: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n",
      "/project/nsaru/nsaru-cv/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:693: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    }
   ],
   "source": [
    "for model in ['mlp']:\n",
    "   #Pipeline\n",
    "    #initial scaling\n",
    "    scaler = StandardScaler()\n",
    "    #Best parameter (CV score=0.892): mlp v1\n",
    "    #{'clf__activation': 'logistic', 'clf__hidden_layer_sizes': (432, 117, 32), 'pca__n_components': 0.999}\n",
    "    #0.8922358777029111 0.8923369140625\n",
    "    #pca\n",
    "    pca = PCA(svd_solver='full',n_components=0.999)\n",
    "    clf = MLPClassifier(max_iter=1000)\n",
    "    if model == 'mlp':\n",
    "        #mlp\n",
    "        n_feat = 69\n",
    "        clf = MLPClassifier(max_iter=1000)\n",
    "        #tune hyperparameters\n",
    "        layers = []\n",
    "\n",
    "        for layer1 in [2,4]:\n",
    "            for layer2 in [4,8]:\n",
    "                layer = (int(layer1*n_feat),int(np.sqrt(layer1*n_feat*layer2*n_components)),layer2*n_components)\n",
    "                layers.append(layer)\n",
    "\n",
    "        #parameters = {'pca__n_components':(0.99,0.999),'clf__activation':('relu','logistic'),'clf__hidden_layer_sizes':layers}\n",
    "        parameters = {'clf__hidden_layer_sizes':layers}\n",
    "    \n",
    "    elif model == 'hgbc':\n",
    "        clf = HistGradientBoostingClassifier(max_iter=1000)\n",
    "    \n",
    "       # parameters = {'pca__n_components':(0.99,0.999),'clf__min_samples_leaf':(20,200,2000,20000),'clf__max_bins':(31,63,127,255)}\n",
    "        parameters = {'clf__max_bins':(31,63,127,255)}\n",
    "    pipeline = Pipeline(steps=[(\"scaler\", scaler), (\"pca\", pca), (\"clf\", clf)])\n",
    "    \n",
    "    search = HalvingGridSearchCV(pipeline, parameters,n_jobs=-1,cv=5,verbose=3,aggressive_elimination=True)\n",
    "    \n",
    "    search.fit(train_feats, train_labels)\n",
    "    print(\"Best parameter (CV score=%0.3f):\" % search.best_score_)\n",
    "    print(search.best_params_)\n",
    "    filename = os.path.join(p3,'model_pipeline_'+version+'_'+model+'_'+datestr+'_final.pk.sav')\n",
    "    with open(filename, 'wb') as f:  # Python 3: open(..., 'wb'\n",
    "        pickle.dump(search.best_estimator_, f)\n",
    "        \n",
    "    pred = search.predict(test_feats)\n",
    "\n",
    "\n",
    "    M,f,a = cornfusion(test_labels,pred,n_components)\n",
    "    \n",
    "    plt.matshow(M)\n",
    "    plt.ylabel(\"Predicted\")\n",
    "    plt.xlabel(\"Observed\")\n",
    "    plt.title(version+\" Confusion Matrix\")\n",
    "    plt.savefig(os.path.join(p3,'m_'+version+'_'+model+'_'+datestr+'_final.png'),dpi=300)\n",
    "    \n",
    "    print(f,a)\n",
    "    \n",
    "    M_df = {}\n",
    "    M_df['sun_soil'] = M[:,0]\n",
    "    M_df['sun_res'] = M[:,1]\n",
    "    M_df['sun_can'] = M[:,2]\n",
    "    M_df['sun_snow'] = M[:,3]\n",
    "    M_df['shade_soil'] = M[:,4]\n",
    "    M_df['shade_res'] = M[:,5]\n",
    "    M_df['shade_can'] = M[:,6]\n",
    "    M_df['shade_snow'] = M[:,7]\n",
    "    M_df = DataFrame(M_df)\n",
    "    M_df.to_csv(os.path.join(p3,'M_'+version+'_'+model+'_'+datestr+'_final.csv'))\n",
    "    \n",
    "    p_df = DataFrame(search.best_params_)\n",
    "    p_df.to_csv(os.path.join(p3,'params_'+version+'_'+model+'_'+datestr+'_final.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e767a3-5138-4012-a549-a8443c890ede",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
